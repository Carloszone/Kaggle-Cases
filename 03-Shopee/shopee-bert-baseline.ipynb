{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from shutil import copyfile\nimport re\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport warnings\nwarnings.simplefilter('ignore')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras import backend as K\ncopyfile(src = \"../input/shoppee-modles/tokenization.py\", dst = \"../working/tokenization.py\")\nimport tokenization\nimport tensorflow_hub as hub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration\nEPOCHS = 15\nBATCH_SIZE = 30\n# Seed\nSEED = 9527\n# Verbosity\nVERBOSE = 1\nLR = 0.00001","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n    \ndef read_and_preprocess():\n    df = pd.read_csv('../input/shopee-product-matching/train.csv')\n    tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n    df['matches'] = df['label_group'].map(tmp)\n    df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n    encoder = LabelEncoder()\n    df['label_group'] = encoder.fit_transform(df['label_group'])\n    N_CLASSES = df['label_group'].nunique()\n    print(f'We have {N_CLASSES} classes')\n    x_train, x_val, y_train, y_val = train_test_split(df[['title']], df['label_group'], shuffle = True, stratify = df['label_group'], random_state = SEED, test_size = 0.33)\n    return df, N_CLASSES, x_train, x_val, y_train, y_val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for a custom learning rate scheduler with warmup and decay\ndef get_lr_callback():\n    lr_start   = 0.000001\n    lr_max     = 0.000005 * BATCH_SIZE\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max    \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n    return lr_callback\n\n# Return tokens, masks and segments from a text array or series\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\n\n# Arcmarginproduct class keras layer\nclass ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https://arxiv.org/pdf/1801.07698.pdf\n        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n            blob/master/src/modeling/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output\n\n# Function to build bert model\ndef build_bert_model(bert_layer, max_len = 512):\n    \n    margin = ArcMarginProduct(\n            n_classes = N_CLASSES, \n            s = 30, \n            m = 0.5, \n            name='head/arc_margin', \n            dtype='float32'\n            )\n    \n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    label = tf.keras.layers.Input(shape = (), name = 'label')\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    x = margin([clf_output, label])\n    output = tf.keras.layers.Softmax(dtype='float32')(x)\n    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\n    model.load_weights('../input/shoppee-modles/Bert_9527.h5')\n    model.compile(optimizer = tf.keras.optimizers.Adam(lr = LR),\n                  loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n                  metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n    return model\n\ndef load_train_and_evaluate(x_train, x_val, y_train, y_val):\n    seed_everything(SEED)\n    # Load BERT from the Tensorflow Hub\n    module_url = \"../input/shoppee-modles/bert_en_uncased_L-24_H-1024_A-16_2\"\n    bert_layer = hub.KerasLayer(module_url, trainable = True)\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n    x_train = bert_encode(x_train['title'].values, tokenizer, max_len = 70)\n    x_val = bert_encode(x_val['title'].values, tokenizer, max_len = 70)\n    y_train = y_train.values\n    y_val = y_val.values\n    # Add targets to train and val\n    x_train = (x_train[0], x_train[1], x_train[2], y_train)\n    x_val = (x_val[0], x_val[1], x_val[2], y_val)\n    bert_model = build_bert_model(bert_layer, max_len = 70)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'Bert_{SEED}.h5', \n                                                    monitor = 'val_loss', \n                                                    verbose = VERBOSE, \n                                                    save_best_only = True,\n                                                    save_weights_only = True, \n                                                    mode = 'min')\n    history = bert_model.fit(x_train, y_train,\n                             validation_data = (x_val, y_val),\n                             epochs = EPOCHS, \n                             callbacks = [checkpoint, get_lr_callback()],\n                             batch_size = BATCH_SIZE,\n                             verbose = VERBOSE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df, N_CLASSES, x_train, x_val, y_train, y_val = read_and_preprocess()\nload_train_and_evaluate(x_train, x_val, y_train, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}