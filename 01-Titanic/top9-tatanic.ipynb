{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport seaborn as sn\nimport xgboost as xgb\nfrom statistics import mode\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data set\ndf_train = pd.read_csv('../input/titanic/train.csv')\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('../input/titanic/test.csv')\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleansing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing value in df_train\ndf_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace NA in Age with the median\ndf_train.Age = df_train.Age.fillna(df_train.Age.median())\n\n# replace NA in Cabin with 0\ndf_train.Cabin = df_train.Cabin.fillna(0)\n\n# replace NA in Embarked with the mode\ndf_train.Embarked = df_train.Embarked.fillna(mode(df_train.Embarked))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing value again\ndf_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing value in df_test\ndf_test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace NA in Age with the median\ndf_test.Age = df_test.Age.fillna(df_test.Age.median())\n\n# replace NA in Cabin with 0\ndf_test.Cabin = df_test.Cabin.fillna(0)\n\n# replace NA in Fare with the mean\ndf_test.Fare = df_test.Fare.fillna(np.mean(df_test.Fare))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing value in df_test agian\ndf_test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"Now, the task is generate as many features as possible. here is my plan:<br>\n[1] Name. generate two features: name length and title, and title will be transform into dummy variables<br>\n[2] SibSp and Parch. For me, they are very similar, so I can create a new feature called family which equals SibSp + Parch, or I can set several bins for the two columns (0, 1, or 2+)<br>\n[3] Ticket. generate two features: ticket length and letter (the ticket number contains letter or not)<br>\n[4] Cabin. two features: Cabin letter (dummy variable) and Cabin number<br>\n[5] Fare. based on Cabin number, generate ave_fare = Fare / Cabin number <br>\n[6] Embarked. Transform into dummy variables <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set a class to generate features\nclass data(object):\n    def __init__(self,dataset):\n        self.dataset = dataset.copy()\n    def new(self):\n        self.dataset['Sex'] = self.dataset.Sex.replace(\"female\", 0)\n        self.dataset['Sex'] = self.dataset.Sex.replace(\"male\", 1)\n        self.dataset['name_length'] = self.dataset.Name.str.len()\n        self.dataset['title'] = self.dataset.Name.str.extract(pat='([a-zA-Z]+\\.)')\n        self.dataset['family'] = self.dataset.SibSp +self.dataset.Parch\n        self.dataset['ticket_length'] = self.dataset.Ticket.str.len()\n        self.dataset['ticket_letter'] = self.dataset.Ticket.apply(lambda x : 1 if bool(re.search('[A-Za-z]+',x)) else 0)\n        self.dataset['cabin_letter'] = self.dataset.Cabin.str.replace('[0-9]+','').str.replace(' ','')\n        self.dataset['cabin_number'] = self.dataset.cabin_letter.str.len()\n        self.dataset['cabin_number'] = self.dataset.cabin_number.apply(lambda x: 1 if x != x else x)\n        self.dataset['ave_fare'] = self.dataset.Fare / self.dataset.cabin_number\n        self.dataset = self.dataset.drop(columns = ['PassengerId', 'Name', 'Ticket', 'Cabin'])\n        return self.dataset\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train = data(df_train).new()\nnew_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# violinplot of Pclass\nsn.violinplot(x=\"Survived\", y=\"Pclass\", data=new_train, size=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# violinplot of Sex\nsn.violinplot(x=\"Survived\", y=\"Sex\", data=new_train, size=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kdeplot of Age\nsn.FacetGrid(new_train, hue=\"Survived\", height=6) \\\n   .map(sn.kdeplot, \"Age\") \\\n   .add_legend()\n\n# I plan to create severals bins for Age: 0-18, 18-30, 30-60, 60+","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SibSp, Parch, and  family\nsn.jointplot(x=\"SibSp\", y=\"Parch\", data=new_train, height=5)\nplt.show()\n\nsn.FacetGrid(new_train, hue=\"Survived\", size=6) \\\n   .map(sn.kdeplot, \"family\") \\\n   .add_legend()\n\n# bins for family: 0, 1-3, 4+","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kdeplot of Fare, ave_fare\nsn.histplot(new_train, x ='Fare')\nplt.show()\n\nsn.histplot(new_train, x = 'ave_fare')\nplt.show()\n\nsn.FacetGrid(new_train, hue=\"Survived\", height=6) \\\n   .map(sn.kdeplot, \"Fare\") \\\n   .add_legend()\nplt.show()\n\nsn.FacetGrid(new_train, hue=\"Survived\", height=6) \\\n   .map(sn.kdeplot, \"ave_fare\") \\\n   .add_legend()\nplt.show()\n\n# higher fare can bring higher survivied possibility","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sn.violinplot(x=\"Embarked\", y=\"Survived\", data=new_train, height=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sn.violinplot(x = 'Survived', y = 'title', data = new_train, height = 6)\n\n# five groups: Mr., Mrs., Miss., Master., other","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sn.violinplot(x = 'Survived', y = 'name_length', data=new_train, height=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sn.violinplot(x = 'Survived', y = 'ticket_letter', data=new_train, height=6)\n\n#there is no siginificant difference, so the feature is not good","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sn.violinplot(x = 'Survived', y = 'ticket_length', data=new_train, height=6)\n\n#there is no siginificant difference, so the feature is not good","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sn.violinplot(x = 'Survived', y = 'cabin_letter', data = new_train, height=6)\n\n# people in B C D E F cabin has a higher survived rate,in A ,G and other are not","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**<br>\n[1] Pclass:keep<br>\n[2] Sex: keep<br>\n[3] Age: bins(0-18,18-30,30-60,60+)<br>\n[4] SibSp: remove<br>\n[5] parch: remove<br>\n[6] family: bins(0,1-3,4+)<br>\n[7] Fare, ave_fare: keep one<br>\n[8] Embarked: keep, dummy<br>\n[9] ttile: bins(Mr., Mrs., Miss., Master., other)<br>\n[10] name_length: keep<br>\n[11] ticket (letter and length): remove<br>\n[12] Cabin_letter: bins (BCDEF, AG, other)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimize the class to generate features 1\nclass data2(object):\n    def __init__(self,dataset):\n        self.dataset = dataset.copy()\n    def new(self):\n        self.dataset['Sex'] = self.dataset.Sex.replace(\"female\", 0)\n        self.dataset['Sex'] = self.dataset.Sex.replace(\"male\", 1)\n        self.dataset['Age'] = pd.cut(self.dataset['Age'], [0,18,30,60,100], labels = ['bin1', 'bin2', 'bin3', 'bin4'])\n        self.dataset['family'] = self.dataset.SibSp +self.dataset.Parch\n        self.dataset['family'] = pd.cut(self.dataset['family'], [-0.5,0.5,3.5,np.Inf], labels = ['b1','b2','b3'])\n        self.dataset['title'] = self.dataset.Name.str.extract(pat='([a-zA-Z]+\\.)')\n        for titl in ['Master.','Mrs.','Miss.','Mr.']:\n            self.dataset[titl] = self.dataset.title.apply(lambda x: int(titl in x if isinstance(x, str) else False))\n        self.dataset['title_Other'] = self.dataset.loc[:,['Master.','Mrs.','Miss.','Mr.']].sum(1)\n        self.dataset['title_Other'] = self.dataset.title_Other.apply(lambda x : 1 if x == 0 else 0)\n        self.dataset['name_length'] = self.dataset.Name.str.len()\n        self.dataset['cabin_letter'] = self.dataset.Cabin.str.replace('[0-9]+','').str.replace(' ','')\n        self.dataset['cabin_letter'] = self.dataset.cabin_letter.apply(lambda x : 'other' if not(isinstance(x, str)) else \\\n                                                                       'b1' if any(['A'in x, 'G' in x]) else \\\n                                                                      'b2' if any(['B' in x, 'C' in x, 'D' in x, 'E' in x, 'F' in x]) else 'other')\n        self.dataset = pd.get_dummies(self.dataset, columns = ['Embarked', 'Pclass', 'Age', 'family', 'cabin_letter'])\n        self.dataset = self.dataset.drop(columns = ['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin','title'])\n        return self.dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimize the class to generate features 2\nclass data3(object):\n    def __init__(self,dataset):\n        self.dataset = dataset.copy()\n    def new(self):\n        self.dataset['Sex'] = self.dataset.Sex.replace(\"female\", 0)\n        self.dataset['Sex'] = self.dataset.Sex.replace(\"male\", 1)\n        self.dataset['Age'] = self.dataset.Age - self.dataset.Age.mean()\n        self.dataset['Fare'] = self.dataset.Fare - self.dataset.Fare.mean()\n        self.dataset['family'] = self.dataset.SibSp +self.dataset.Parch\n        self.dataset['family'] = pd.cut(self.dataset['family'], [-0.5,0.5,3.5,np.Inf], labels = ['b1','b2','b3'])\n        self.dataset['title'] = self.dataset.Name.str.extract(pat='([a-zA-Z]+\\.)')\n        for titl in ['Mrs.','Miss.','Mr.']:\n            self.dataset[titl] = self.dataset.title.apply(lambda x: int(titl in x if isinstance(x, str) else False))\n        self.dataset['title_Other'] = self.dataset.loc[:,['Mrs.','Miss.','Mr.']].sum(1)\n        self.dataset['title_Other'] = self.dataset.title_Other.apply(lambda x : 1 if x == 0 else 0)\n        self.dataset['name_length'] = self.dataset.Name.str.len()\n        self.dataset['cabin_letter'] = self.dataset.Cabin.str.replace('[0-9]+','').str.replace(' ','')\n        self.dataset['cabin_letter'] = self.dataset.cabin_letter.apply(lambda x : 'other' if not(isinstance(x, str)) else \\\n                                                                       'b1' if any(['A'in x, 'G' in x]) else \\\n                                                                      'b2' if any(['B' in x, 'C' in x, 'D' in x, 'E' in x, 'F' in x]) else 'other')\n        self.dataset = pd.get_dummies(self.dataset, columns = ['Embarked', 'Pclass', 'Age', 'family', 'cabin_letter'])\n        self.dataset = self.dataset.drop(columns = ['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin','title'])\n        return self.dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"class model_selection:\n    def __init__(self, cv, model_info, parameters):\n        self.cv = cv\n        self.model_info = model_info\n        self.parameters = parameters\n    def grid_search(self, x, y):\n        x = np.array(x)\n        y = np.array(y)\n        grid_result = []\n        for model in self.model_info.keys():\n            search_rf = GridSearchCV(estimator = self.model_info[model],\n                                     param_grid = self.parameters[model], \n                                     cv = self.cv,\n                                     n_jobs = -1,\n                                     verbose = 2)\n            search_rf.fit(x, y)\n            grid_result.append(search_rf.best_params_)\n        return  grid_result\n    def scores(self, model_list,train_x, train_y):\n        model_names = []\n        model_score = []\n        for model in model_list:\n            model.fit(train_x, train_y)\n            model_names.append(model)\n            model_score.append(model.score(train_x, train_y))\n        result = pd.DataFrame({'models':model_names, 'socres':model_score})\n        return result\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid Search\nmodel_info = {'rf':RandomForestClassifier(), 'xgb':xgb.XGBClassifier(), 'ert':ExtraTreesClassifier(),\n              'lr':LogisticRegression(), 'knn':KNeighborsClassifier(), 'svc':SVC()}\n\n# Random Forset\ngrid_rf = {\n    \"n_estimators\": np.linspace(100,1000,5, dtype = int),\n    \"max_depth\": [3,5,7],\n    \"max_features\": [3,5,7,9,11],\n    \"min_samples_leaf\": [3,5,7],\n    \"min_samples_split\":[3,5,7],\n    \"random_state\": [2020,2021]\n}\n\ngrid_xgb = {\n    'booster': ['gbtree'],\n    'objective': ['binary:logistic'],\n    'subsample': [0.6,0.7,0.8],\n    'colsample_bytree': [0.6,0.7,0.8],\n    'eta': [0.05,0.1,0.2],\n    'max_depth': [3,5],\n    'seed': [2020, 2021],\n    'eval_metric': ['logloss']\n}\n\ngrid_ert = {\n    \"n_estimators\": np.linspace(100,900,5, dtype = int),\n    \"max_depth\": [3,5,7],\n    \"max_features\": [3,5,6,7],\n    \"min_samples_leaf\": [3,5,7],\n    \"min_samples_split\":[3,5,7],\n    \"random_state\": [2020, 2021]\n}\n\ngrid_lr = {\n    \"penalty\": ['l1', 'l2'],\n    \"C\":[0.01, 0.05, 0.1, 0.25, 0.5],\n    \"random_state\": [2020, 2021]\n}\n\ngrid_knn = {\n    \"n_neighbors\": [5,7,10,15,20]\n}\n\ngrid_svc = {\n    \"C\":[0.001, 0.005, 0.01, 0.05, 0.1, 0.25, 0.5],\n    \"gamma\":[0.001, 0.005, 0.01, 0.05, 0.1, 0.25, 0.5],\n    \"random_state\": [2020, 2021]\n}\n\nparameters = {'rf':grid_rf, 'xgb':grid_xgb, 'ert':grid_ert,\n              'lr':grid_lr, 'knn':grid_knn, 'svc':grid_svc}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train = data2(df_train).new()\nnew_test = data2(df_test).new()\n\ntrain_x = new_train.drop(columns = 'Survived')\ntrain_y = new_train.Survived\ntest_x = new_test\n\n(best_rf_1, best_xgb_1, best_ert_1, best_lr_1, best_knn_1, best_svc_1) = model_selection(3, model_info, parameters).grid_search(train_x, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create models\nmodel_rf = RandomForestClassifier(**best_rf_1)\n\nmodel_xgb = xgb.XGBClassifier(**best_xgb_1)\n\nmodel_ert = ExtraTreesClassifier(**best_ert_1)\n\nmodel_lr = LogisticRegression(**best_lr_1)\n\nmodel_knn = KNeighborsClassifier(**best_knn_1)\n\nmodel_svc = SVC(**best_svc_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_list = [model_rf, model_xgb, model_ert, model_lr, model_knn, model_svc]\nmodel_selection(3, model_info, parameters).scores(model_list,train_x, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train_2 = data3(df_train).new()\nnew_test_2 = data3(df_test).new()\n\ntrain_x_2 = new_train_2.drop(columns = 'Survived')\ntrain_y_2 = new_train_2.Survived\ntest_x_2 = new_test_2\n(best_rf_2, best_xgb_2, best_ert_2, best_lr_2, best_knn_2, best_svc_2) = model_selection(3, model_info, parameters).grid_search(train_x_2, train_y_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create models\nmodel_rf = RandomForestClassifier(**best_rf_2)\n\nmodel_xgb = xgb.XGBClassifier(**best_xgb_2)\n\nmodel_ert = ExtraTreesClassifier(**best_ert_2)\n\nmodel_lr = LogisticRegression(**best_lr_2)\n\nmodel_knn = KNeighborsClassifier(**best_knn_2)\n\nmodel_svc = SVC(**best_svc_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_list = [model_rf, model_xgb, model_ert, model_lr, model_knn, model_svc]\nmodel_selection(3, model_info, parameters).scores(model_list,train_x_2, train_y_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble Generation"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ensemble:\n    def __init__(self, cv, base_models, target_model):\n        self.cv = cv\n        self.models = base_models\n        self.target = target_model\n    def predict(self, train_x, train_y, test_x):\n        train_x = np.array(train_x)\n        train_y = np.array(train_y)\n        test_x = np.array(test_x)\n        kf =KFold(n_splits = self.cv, shuffle=True, random_state=2021)\n        result1 = np.zeros((train_x.shape[0],len(self.models)))\n        result2 = np.zeros((test_x.shape[0],len(self.models)))\n        for ind1, model in enumerate(self.models):\n            result3 = np.zeros((test_x.shape[0], self.cv))\n            for ind2, (train_index, test_index) in enumerate(kf.split(train_x)):\n                f_x = train_x[train_index,:]\n                f_y = train_y[train_index]\n                s_x = train_x[test_index,:]\n                model.fit(f_x,f_y)\n                pred1 = model.predict(s_x)[:]\n                pred2 = model.predict(test_x)[:]\n                result1[test_index,ind1] = pred1\n                result3[:,ind2] = pred2\n            result2[:,ind1] = result3.mean(1)\n        self.target.fit(result1,train_y)\n        y_pred = self.target.predict(result2)[:]\n        return y_pred\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_models = [model_rf, model_xgb, model_ert, model_lr, model_knn, model_svc]\ntarget_ert = xgb.XGBClassifier()\n\ntarget_model = target_ert","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = ensemble(5, base_models, target_model).predict(train_x, train_y, test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = pd.DataFrame({'PassengerId':df_test.PassengerId, 'Survived': pred})\nres.to_csv('result_stacking.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}